<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[mobilenet 总结]]></title>
    <url>%2F2020%2F06%2F02%2Fmobilenet%2F</url>
    <content type="text"><![CDATA[MathJax.Hub.Config({ tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]} }); Mobile NetKeypoints: 将标准的卷积操作拆分成了Depthwise Convolution和Pointwise Convolution 总结了神经网络宽度变化$\alpha$以及分辨率变化$\rho$对准确率的影响。此处暂且不讨论。 卷积拆分模型size和计算量对比 Standard Convolution Depthwise Convolution Pointwise Convolution Kernel Size $D_K\times D_K\times M\times N$ $D_K\times D_K \times M$ $M\times N$ Computation Cost $D_K\cdot D_K\cdot M\cdot N\cdot D_F\cdot D_F$ $D_K\cdot D_K\cdot M\cdot D_F\cdot D_F$ $M\cdot N\cdot D_F\cdot D_F$ Ratio/standard 1 $\frac{1}{N}$ $\frac{1}{D_K^2}$ 相关公式: Standard Convolution: $\mathbf{G}{k, l, n}=\sum{i, j, m} \mathbf{K}{i, j, m, n} \cdot \mathbf{F}{k+i-1, l+j-1, m}$ Depthwise convolution: $$\hat{\mathbf{G}}{k,j,m}=\sum{i,j}\hat{\mathbf{K}}{i,j,m}\mathbf{F}{k+i-1,l+j-1,m} $$ Pointwise convolution(1x1 Convolution): $\mathbf{G}{k,l,n}=\sum{m} \hat{\mathbf{G}}{k,l,m}\mathbf{K}{m,n}$ Combination: $\begin{aligned}&amp;\mathbf{G}{k, l, n}=\sum{m}\left(\sum{i=1} \hat{\mathbf{K}}{i, j, m} \cdot \mathbf{F}{k+i-1,l+j-1,m}\right) \mathbf{K}{m, n}\&amp;=\sum{m} \sum{i,j}\left(\hat{\mathbf{K}}{i,j,m} \mathbf{K}{m, n}\right) \mathbf{F}_{k+i+1, l+j-1, m}\end{aligned}$ 理解：将Standard Convolution拆开实际上是一种解耦，认为使用 $\mathbf{K}{i,j,m,n}=K(i,j,m,n)-&gt;K(i,j,m)\cdot K(m,n)=K{i,j,m}\cdot K_{m,n}$ Kernel 关于模型的具体结构： 此处在Depthwise Conv和1x1 Conv后面添加了BN和ReLU，但在公式的拆分中没有体现，值得思考为何Depthwise之后也需要添加BN和ReLU。 MobileNet2MobileNetV2在V1的基础上进行了改进，主要提出了两种新的结构：Linear Bottlenecks, Invert Residuals. Linear Bottlenecks首先注意到2个性质: If the manifold of interest remains non-zero volume after ReLU transformation, it corresponds to a linear transformation. ReLU(x) = x, x&gt;0 当值大于0时，ReLU相当于线性 ReLU is capable of preserving complete information about the input manifold, but only if the input manifold lies in a low-dimensional subspace of the input space. 当输入维度比较低的时候，ReLU可以保留较为完整的信息 据此，在原先的结构中添加一层Linear Bottlenecks层: 将维度先压缩，然后再通过ReLU，再使用1x1卷积增维, 如下图所示。 模型结构: Inverted Residuals在bottleneck处进行skip connection，内存消耗小，且在该论文的实验中效果稍好。]]></content>
      <categories>
        <category>Deep_learning</category>
      </categories>
      <tags>
        <tag>lightweight_network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[谈死亡]]></title>
    <url>%2F2019%2F11%2F24%2Fabout_death%2F</url>
    <content type="text"><![CDATA[其实早在看完哪吒电影的时候就想开始这样一篇关于死亡的文章。看见哪吒得知自己三年之后就要死的时候，不禁想起了小时候突然意识到我自己有一天也会死去，以至于后来思考了很长时间这个问题。文章写了很久依旧不是很满意，很多逻辑可能都还没有自洽，毕竟这个问题我也还未思考到彻底清晰。不过再拖下去，恐怕永远也写不出来了。所以尽管充满问题，但也先写出来吧，等着以后有了更好的认知和文笔再慢慢修改。 第一次意识到死亡是在小时候的一场梦里。我梦见了自己的死亡。正是从那时候开始，我开始真正意识到我会死去，每个人都需要面对死亡。事实上，那时候的我也并未真正意识到死亡是什么，但却好像对死亡有一种莫名的恐惧。这种恐惧或许是源自于数百万年前时刻面对着死亡威胁的祖先们留下来的基因，抑或是因为数千年来统治者为加强统治而刻意宣传的死亡恐怖的文化。总之，这样一种恐惧在我的心底里埋藏了数年之久，这也令那个年纪的我感到羞愧不已。毕竟对于一个小男孩而言，向往的都是一些勇敢不怕死的英雄。 再到后来，我试图向这种恐惧发起反抗，反抗的工具就是思考。我试图开始思考死亡的本质。 死亡究竟是什么？有人说，人有三次死亡，第一次是肉体的死亡，然后是葬礼上亲人的告别，最后是遗忘，是这个世界一切与你有关的事物都开始磨灭。这三次死亡，层层递进，营造出了一种悲凉的气氛。但可怕吗，在我在看来似乎也没有那么可怕。毕竟人死一切成空，思维和感官都不再存在，被遗忘又有什么可怕呢。反倒是活着的人，被遗忘似乎来得更痛苦吧。 当然，也有许多人宣称这个世界或许并非是物质的。肉体的死亡接着的是精神体的延续。事实上东西方的文化里，都存在类似于精神灵魂的说法，而且都需够了地狱用以惩罚那些生前不符合某些人统治的灵魂。借此来宣传死亡的可怕。这种可怕是利用了人们对死后世界的未知。但对许多人而言，活着又何尝不是被统治被约束，被惩罚着呢。死亡也只是换了一个地方而已。从这点看来，死亡和活着似乎也没那么大区别。 那么，在本质上死亡可能会意味着两种状态，一种是空，另一种无非是换一个地方以另一种形态“活着”。当然也可能处于一些未知的奇异状态，暂且不做探讨。 这么想来，死亡似乎也没那么可怕，死亡与活着最大的区别在于一个未知，一个已知，一个未来，一个已来。活着对我们来说是已知的状态，而它恰巧又是死亡的反面。那么，我们不妨换个角度来看问题，谈谈活着。 活着对不同的人而言有着不同的意义。对于第一类人来说，活着是一种本能。对他们来说活着就是全部，就是第一要义。他们对于死亡的恐惧是出于想要活着的本能。 而第二类人，活着对他们来说是快乐的。这样的人无疑是幸运的，上帝给了他们足够幸福的生活，又关闭了他们对于一些痛苦的感知。他们把生老病死都可以当做是人生的一种快乐的体验。对这类人而言，人间是乐园。他们对死亡的恐惧是出于对美好幸福的生活的眷恋。 与第二类人相比，第三类人无疑是不幸的。他们的人生并不那么幸福，他们或是饱经战乱，或是饱受身体的病痛与情感的挣扎，或是对世间的痛苦足够敏感。活着对于他们来说太过困难。死亡对与他们来说不在是恐惧，甚至是解脱，他们对活着的坚守或是出于责任，或是出于亲人朋友的情感羁绊。对这类人而言，在这个世界的生存举步维艰，他们靠着微弱的联系或支撑在这个世界上生存，一旦被破坏，死亡就成了他们的解脱与归宿。但人类实在是很复杂，有许多第三类人找到了自我救赎的钥匙，这或许是出于生存的本能，或许是人类人性的光辉。他们有人开始逐渐将人生的一切苦难当做一种体验，用强大的精神能量扭转现实的不幸，有人将活着视作自己的武器，以更加向上与高傲的姿态面对苦难的人生，以此反抗命运的不公。 这三类人，其实也并不能完全分开，事实上很难找到一个人的人生是绝对的快乐，也很难找到一个人的人生完全是痛苦，而本能更是每个人都很难摆脱的，只有程度不同的问题。 而在这三类人之外的第四类人，他们不在局限于活着本身，而是去思考活着真正的意义。他们顺着因果链一路向上，发现了虚无荒诞的本质。他们中有人认为这种荒诞的人生是众神的惩罚，于是以自杀向众神发起反抗，宣誓着对自我存在的主权。也有人带着对众神的蔑视，以积极的态度面对对抗虚无与荒诞。更有人，喊出上帝已死，认为虚无是人性的解放，人可以做自己的上帝，在一片虚无中创造出新的意义，并以饱满的态度用生命去践行意义。对于这类人而言，活着与死亡本质已不再重要，更重要的是其背后的意义。 死亡是一切的终点。每个人都会死，甚至人类文明也可能会凋亡。我们终有一天要去面对死亡。而当我们真正开始思考死亡的时候，也会开始思考活着的价值，思考如何去更好的活着。我们或是用力去体验这花花世界美好人生，或是在苦难中自我救赎，或是在虚无荒诞的人生中做自己的上帝。我们不必畏惧死亡，但既然已经活着了，那不如就以最好的姿态活着。这是对生命的尊重，也是对死亡的尊重。]]></content>
      <categories>
        <category>books</category>
      </categories>
      <tags>
        <tag>philosophy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[演员自我修养阅读心得]]></title>
    <url>%2F2019%2F10%2F28%2Factor-book%2F</url>
    <content type="text"><![CDATA[有的时候伟大作品背后的整个体系可能比这些作品本身更有意思。去了解一个艺术背后的体系，首先可以提升我们对艺术的鉴赏和理解能力。另一方面，每一个成熟的体系都是无数先人的智慧结晶，从这样一个体系中，我们也可以借鉴到许多不同的思路与智慧，应用到日常生活中。 尤其像表演艺术这种与生活密切相关的艺术形式，能够提供许多生活上值得参考的地方。我们可以从中学习如果去真正的理解一个人，就像演员去理解一个角色一般，还可以学会如何去细致的观察生活，发现生活中的细节和美以及乐趣。这也是我认为的表演艺术一大精华，就是发现不一样的生活，要有一双艺术家的眼睛。 表演艺术与其他一些艺术或者学科不同的地方在于，表演的最终目的是让别人看不出你在表演，也就是真实感。一段优秀的表演是潜意识的，是不着痕迹的。但很显然潜意识并不受我们所控制。所以对于表演艺术而言，需要有一套技术和方法，去充分的模拟甚至是挖掘我们的潜意识。但是就本书而言其实并没有对潜意识有一个成体系的分析，只是强调了潜意识的重要性，同时提出了一些有趣的技法来帮助调用潜意识。当然这些技术对我帮助并不大，也就不做整理。 这里主要摘录一些对我个人有帮助的片段以及一些理解。 灵感:“一个人不可能总是潜意识地依靠灵感进行创作。世上没有这样的天才。因此，戏剧这门艺术首先教我们有意识地恰当地创作，因为这将是潜意识绽放、灵感出现最好的准备方式。在塑造角色时，进行有意识地创作的时间越多，就越有可能激发情感自然地流露。”我们不必去刻意的追求灵感，我们也很难的主动的去创造灵感，但是我们可以创造产生灵感的沃土，也可以训练一种能力让我们在灵感到来的时候，紧紧的抓住它，将它发挥到极致。我们可以辅以相应的技术来帮助我们利用灵感。 理解角色与理解一个人 “首先你应该透彻地了解角色。这是十分复杂的。你可以从人物所生活的时代、国家、生活条件、社会背景、文学修养、精神、心理、生活方式、社会地位和外貌形象进行研究；而且，你要研究人物的特征，如个人习惯、举止、动作、声音、说话方式、说话腔调等。这其实也是我们如何去理解一个现实中的人物的方式，不要轻易的以表面去判断一个人。要尽可能的收集他的信息，对信息建立合理的猜测和推论，同时建立联系。即通过一切现象分析本质，再根据推论去推演这个人的行为方式，不断修正推论，建立基本模型Person(x;t)。 想象力 学会诱导和锻炼自身的想象力，从一个规定情境出发。用假如来唤醒想象力。即考虑一个现实的场景，然后先调整其中一个变量，再顺着逻辑去推演。或者考虑一个现实场景，然后用想象力去填充现实场景的细节，可以建立一些合理的假设，用内心的力量改变周围的物质世界。 想象细节：艺术里不允许有“大概”这种概念，必须要考虑好所有的细节。要让它们始终是有逻辑的，连贯的。这将帮助你抓住那些不现实不稳定的梦幻，让它们靠近稳定的可靠的现实。 赋予想象灵魂：通过有意识的理性的方法去激发想象力往往呈现出毫无生机的生活画面。需要考虑情感细节。去思考情景中带有的情感符号和人物的内心情感。 主动性在想象中至关重要。首先是内在的，其次才是外在的。 到最后，你会对只作为一个旁观者感到厌烦，希望也参与到其中去。那样，作为一个想象生活中的积极 参与者，你将不再看到你自己，而只是看到你周围的环境，你会在内心对周围的一切有所反应，因为你 真正地参与其中 内心注意力内心注意力是以在虚拟环境下我们所看到、听到、触摸到和感觉到的事物为中心 。 “当四周一片黑暗，只有一个亮点的时候，”他说，“在光点内的所有物体都吸引了你们的注意力，因为光点外的一切都是看不到的，对你们没有吸引力。这样一个光圈的轮廓很清晰，四周的黑暗也很浓重， 所以你们没有想去打破这种界限的冲动。 “当灯亮得多了，你们所遇到的问题就完全不同了。因为这个 光圈没有了明显的轮廓，你们就得靠自己在心理上预设出这样一个范围来，而且还不能让自己看到这个 范围之外去。现在你们的注意力必须要代替灯光，尽管在灯光的范围之外有各种各样可以看到的东西在 吸引着你们，也要把自己限定在一定的范围之内。 学会观察普通人不知道如何通过观察面部表情、眼神、声调来领会和他们交谈的人的思想感受。他们既不能主动 地领悟复杂的生活真谛，也不会用心聆听他们所听到的声音。如果他们能够做到这一点，生活，对于他 们来说，就会变得更美好、更轻松，他们的创造性工作也会发生难以估量的变化，变得更加丰富、精致 和深刻。但是一个人生来没有的东西，是不能强加到他身上的，只能尽力发展他可以拥有的能力。培养 注意力要做大量的工作，要花费很长的时间，要拥有成功的欲望，要下功夫进行系统的 。 对于不擅长观察的人，我们怎样才能教会他们关注自然和生活的启迪呢?首先，我们必须教会他们观看和聆听美好的事物。这样的习惯会使他们的思想变得高尚，激发他们的情感，在他们的情感记忆中留下 深刻的印象。在生活中，没有什么比自然更美好了，自然应该成为我们时刻观察的 。 真正的美是丝毫不畏惧缺陷的。事实上，缺陷常常使美更加突出。 “寻找一些美好的和丑陋的事物，对 它们加以描述，学着去认识它们，正确对待它们。否则你对于美的概念会是残缺不全、矫揉造作、虚假 粉饰和青涩感伤。 当你学会了如何观察身边的生活，并从中寻找创作素材之后，你就要去研究最不可或缺、最重要和最鲜活的情感素材，它们是我们主要的创作。 当你通过仔细观察，透过某人的行为、想法和念头清楚了解了他的内心世界之后，要密切关注他的行 为，研究他做出这种行为的环境。他为什么要这样做或者那样做?他当时是怎么想的? “通常我们无法 通过确切的数据了解我们想要了解的人的内心世界，只能借助于直觉走进它。在这里，我们利用的是最细腻和集中的注意力以及本质上是潜意识的观察力。而一般的注意力是不足以深入到他人的内心深处去感知他们的精神生活。 信念女芭蕾舞演员每天都要做基本功的训练，每天都会累得汗流浃背，这样她才能在晚上的演出中表现出优 雅的轻盈之感。歌手每天早晨都要吊嗓子、发长音、增加肺活量、练习颅腔和鼻腔共鸣，这样才能在晚 上的演出中唱出灵魂的心声。没有一个艺术家会忽视必要的技术练习的，他们要依靠必要的技术练习使 自己的身体器官处于最佳 。 那些总是喜欢谈论崇高事情的人，在很大程度上，正是在提升自己方面最无所作为的人。他们带着虚假 的情感谈论艺术和创作，结果却让人感到模糊不清、晦涩难懂。相反，真正的艺术家则是用简洁明了、 通俗易懂的语言来谈论自己的艺术和创作的。仔细思考一下我说的这些话，也想一想这个事实:在某些 角色中，你可以成为不错的演员，也可以成为对艺术做出有益贡献的。 ##适应 你必须学会适应环境、时间和人。如果要求你和一个笨人打交道，你必须根据他的智商做出调整，找到 最简便的方式，了解他的想法。如果一个人很精明，你需要更加谨慎行事，使用更为微妙的方式，以便 他不会看穿你的计谋。 我们的潜意识自有其逻辑。我们已经发现，潜意识适应在我们的艺术中至关重要，因此我会详细讨论它 们。 “最有力、生动、令人信服的适应是神奇艺术家天性的产物。它们几乎全部来源于潜意识。我们发 现最伟大的艺术家会使用它们。但是，这些非凡之人也无法随心所欲地创造它们。它们往往是灵光一 现。有时候，这些适应只是部分潜意识。考虑到只要我们在舞台上就需要与他人不停地沟通，因此我们 彼此之间的调整也是不间断的。想想看，这会有多少动作和走位，其中又包含多少潜意识时刻。 内在驱动力&amp;内在创作状态钢琴家怎么表达情感?他运用钢琴。画家怎么表现?他使用画布、画笔和颜料。因此，演员也借助于他 的精神和肢体创作工具。他的思想、意志和情感结合起来，利用所有的内在‘元素’进行创作。 “陀思妥耶夫斯基一生都在朝圣，受此推动，他写下《卡拉马佐夫兄弟》。托尔斯泰一生都在为自我完善 而努力。安东·契诃夫与资产阶级的平庸生活做斗争，这也成了他大部分文学作品的主题。 “伟大作家 的这种宏观终极目标可以激发演员的所有创作才能，可以凝聚一幕剧或一个角色的所有细枝末节，你们 是否感受到了这种力量呢？ 最高任务所有的细线都朝向同一个目标，汇入一个主要方向。 如果角色中的所有次要任务方向各异，当然无法构建坚实、连续的线。因此，表演就会碎片化、不协 调，与整体脱节。不管各个部分各自如何优秀，在剧中却起不到应有的作用。 由此得出的结论是:最重要的是保护你的最高任务和贯穿动作线。警惕各种与主题无关的额外倾向和目标。 潜意识状态我的‘体系’从不产生灵感，它只造就产生灵感的沃土。 “如果我是你，我会放弃追 逐灵感的幽灵。把灵感交给神仙，让自己投身于人类意识可以控制的范围。 “角色上了正路，就会顺利 前进。它会变得更加广阔、深刻，最终带来灵感。 关注所有可以有意识 控制，并把你带到潜意识的东西。这才是为灵感做了最好的准备。但是，不要直接去寻找灵感，这样会 导致身体僵硬，带来所有不想要的东西。 某个理想主义艺术家，决定将自己献身于某个更为宏大的目标:用高雅的艺术陶冶大众;用诗意的作品 阐释隐藏的精神之美。对于著名的剧本和角色，他会使用新的方式，提供更为本质的内容。他的整个生 命，将献身于这种更高的文化使命。 和人类一样，创作的过程也有一个胚胎期。在创作过程中，父亲是剧本的作者;母亲是孕育角色的演 员;而孩子则是有待出生的角色。 “当演员初次了解所扮演的角色时，这是初期阶段。然后他们变得亲 密、争吵、和解、结婚、怀孕。在这个过程中，导演就像红娘，让这个过程顺利进行。 “在这个时期， 演员会受到角色的影响，也会影响他们的日常生活。偶然情况下，一个角色的孕育期和人的孕育时间一 样长，更为常见的则是时间更长。如果你分析这一过程，你就会相信，孕育一个新的事物，无论是生理上还是在想象方面，都需要遵从一定的自然法则。 其他摘录“时间是我们记忆情感的极好过滤器，也是伟大的艺术家。它不仅净化情感，甚至会将痛苦的现实记忆转化为诗歌。“ 你担心你的观察对象缺乏生命力。其实，任何图片、雕像、朋友的照片、博物馆中的物品，都是无生命 的，但是其中包含着创作这些东西的创作者的某些生命。甚至一盏吊灯，在某种程度上，因为我们对其 全情的关注，也具有了生命力。 不要对剧本进行不必要的切分，不要让细节来引导你。用切分出来的大单元来标注‘航道’，这样，就能完整地建立起剧本的整体框架并且使所有的细节都变得完善。 戏剧艺术的基本目标就是通过艺术的手段来塑造人的精神世界。这种艺术所能做到的就是通过惊人的剧场效果，或哀婉生动的画面来进行表演。但是人类微妙而深邃的情感是无法通过这种技术手段表现出来 我也被带到舞台的中央。这不是一次真正的演出。然而，我的脑海里满是自相矛盾的念头。在舞台上，我就要表演，然而我的内心却要求我表现出孤独的状态。有一个我在说要取悦观众，那样他们就不会感到乏味；另一个我又说不要注意他们。我的双腿、胳膊、脑袋和身子，虽然它们都听我指挥，但是它们又好像自作主张地加了一些其他东西进来。你只是很简单地动动胳膊或腿，但是突然地，你全身都扭曲起来，看着就像你在摆姿势准备拍照 。 在舞台上，必须有动作，要么是外部动作，要么是内部。 “不，不要尽力想办法让我们相信第一次你在寻找胸针，”他说，“你甚至都没有想到它。你只是在寻找痛苦，为了痛苦而痛苦。 参考: 《演员自我修养》康斯坦丁·斯坦尼斯拉夫斯基]]></content>
      <categories>
        <category>books</category>
      </categories>
      <tags>
        <tag>actor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[调味圣经基础部分整理]]></title>
    <url>%2F2019%2F08%2F27%2Fcook_tips_seasoning%2F</url>
    <content type="text"><![CDATA[总结主要参考自《调味圣经》这本书，但就这本书而言，更多的是现象的堆叠，对更加本质东西，如物质的化学式，性质，反应方程式以及反应条件等没有体系化的总结。所以只是随便一读，稍微摘录一些，作为一个参考。如果想要更深入的了解调味或者是从分子角度思考厨艺，还需要找更多的参考资料。 基本味：酸甜苦咸鲜 + 辣（痛觉） 熟悉各种调味品的物理化学性质，以及烹饪中常见的化学反应，从性质出发进行分析。如醋，酒容易挥发；谷氨酸与氯化钠生成谷氨酸钠，产生鲜味，因此盐可以提鲜；醇类和油脂类发生酯化反应产生酯类，带有香味，因此做肉类可以加酒；醋促进蛋白质分解成氨基酸，其中又包含谷氨酸与盐结合生产谷氨酸钠，使得肉质鲜美。 常见调料的化学成分，以及物理化学性质。待更新。 厨房常见的化学反应。待更新。 书籍摘录1. 调味必知常识常见调味品 盐不仅为菜肴赋予基本的咸味，而且还有突出酸、甜味和提鲜的作用。盐的化学成分是氯化钠。氯在人体中的重量百分比为 1. 8，钠为 2. 6。氯化钠在人体新陈代谢中起着重要的作用。氯在人体中以氯离子的形式存在于细胞外液中，它具有维持渗透压、调节酸碱平衡和组成胃酸等生理作用。因此，盐对于人体的健康也起着至关重要的作用。但是盐不可以多食，过量食用会增加心血管疾病的发病率。 醋是杜康的儿子黑塔发明的。其实中国的醋至少在西周时就开始酿造了。食醋能使甜味、咸味变淡，使鲜味更鲜，使辣味缓和。在炖菜时加入一点醋可以加速食材的软化的同时，增添菜肴的香味。醋能消食解毒，是辅佐海鲜的一味不可或缺的蘸料。醋可以促进蛋白质在加热的条件下更多地水解成氨基酸，使烧出的肉味道鲜香、醇厚，同时使肉中的蛋白质更易被人体消化吸收。 味精，学名“谷氨酸钠”。 1907年，日本科学家池田菊苗在吃饭时，对海带汤的鲜度产生兴趣，继而从海带中提取出“谷氨酸钠”，之后与商人合作开发了一种名为“味之素”的调味品。味精可以增进菜肴本味，促进菜肴产生鲜美滋味，并且可起缓解咸味、酸味和苦味，品质好的结晶体味精呈细长的八面棱柱形晶体，颗粒比较均匀，洁白，有光泽，基本透明，无杂质，无其他结晶形态的颗粒。 番茄酱是一种使用成熟番茄制作的色泽红艳、味酸甜的调味品。其风味介于糖醋和荔枝味之间，除直接用于佐餐外，用于菜肴中可增色、增味。番茄酱的制作基础原料有番茄、糖、醋、盐、众香子、洋葱、大蒜、丁香等。 成品甜面酱呈红褐色或黄褐色，有光泽、带酱香、味咸甜适口，黏稠状半流体。一般用于烧、炒、拌类菜肴，主要起增香、增色、解腻的作用。甜面酱的食用方法主要有酱烧菜肴、蘸料，还可以作为配料调味着色。 糖浆常用于为烧烤类菜肴上色，增加光亮，刷上糖浆的原料经烤制后色红润泽，甜香味美，如烤鸭、叉烧肉等。 蜂蜜用于糕点制作，使成品松软爽口、质地均匀、不易翻硬，富有弹性，而且有增白的作用。蜂蜜用于蜜汁菜肴的制作，会产生独特的风味。中式菜肴常用蜂蜜与植物类食材搭配，比如芹菜、百合、莲藕等，做出的菜肴爽甜可口；不过也有专门将蜂蜜加入到荤菜中烧制，以达到滑而不腻的效果。 蜂蜜含量最高的是葡萄糖和果糖，其次是水。一般认为蜂蜜的颜色越浅，透明度越高其质地越好。 蜂蜜的保存尤为讲究，因为蜂蜜是弱酸性的液体，容易和铅、锌、铁等金属发生化学反应，在储藏蜂蜜时应选用陶瓷、玻璃等非金属材料容器，同时，要将蜂蜜和洋葱、大蒜等异味食品分开储存，以防止串味，而且还要防止其与腐蚀性的物质接近，如不要与化肥、石灰等过于接近。 辣椒酱呈赤红色黏稠状，可增添辣味，并增加菜肴色泽。 花椒：苦味；紫红、粒大、肉厚的红袍花椒，不仅麻香味足，而且麻味悠长；鲜花椒气味清香，麻味柔和。在烹调中起去异增香、增麻味的作用。 八角茴香，中国人给了它一个多么好听的名字，不过那是它应得的。除了是一种散寒、理气止痛的好中药之外，它也是一种不可多得的调味料，人们常用以去腥、提味。香味越浓、越辛甜的八角质地越佳。 豆豉是一种用黄豆或者黑豆蒸煮之后发酵而成的调味食品，主要起提鲜、增香、解腻的作用，并具有赋色的功能。 香叶，即月桂叶，香气浓郁，味道辛辣，且略带有植物的苦味，可以用来腌渍食品，又可以用来炖菜，在烹调中有增香去异味，促进食欲的作用。 陈皮既是一味性温和的中药，也是一种理想的调味料，它气味清香，味道兼具酸甜苦辣。在烹调中可以去腥，多与花椒、辣椒为伍。冬日里，煲个汤，放几片陈皮，既美味又养生。陈皮中的各种味道能够通过热汤融入食物内，同时它所具有的温胃散寒、去湿化痰的功效也会随之发挥出来。 肉桂味辛辣，含有一种挥发油，因此芳香馥郁，可以提取出用于香料、化妆品的香精。 五香粉是众多香料的结合，可以增香、去腥、提味。五香粉虽然号称五香，但其实其配料不只限于五种，一般五香粉的基本成分是花椒、八角茴香、肉桂、丁香和小茴香，但是有些配方还包括了胡椒、陈皮、干姜、豆蔻、桂皮等调料，这些都因地域和个人口味而定。五香粉一般被用于炖肉或者海鲜的去腥增味，也常被用作烧烤或者油炸的蘸料。 葱能去除腥味，增添特殊香气，靠的是它本身含有的刺激性挥发油和葱蒜辣素。 姜性温，有芳香，味辛辣，能治感冒轻症，在烹制海鲜时，生姜是一个好帮手，除了能够去除腥味之外，它还能解鱼蟹的“毒”。 蒜有大蒜、小蒜之分，中国原产有小蒜，大蒜原产于欧洲南部和中亚，西汉张骞出使西域，带回来了大量土产，其中就包括葡萄、石榴、黄瓜、芝麻、大蒜、核桃这些农产品。 大蒜味辣，有刺激性气味，在烹调中能起调味、杀菌、去腥、增香的作用。大蒜富含硫元素和氨基酸、肽类和酶类，能防止高血压、动脉硬化等心血管疾病，而且在其所含的 100多种成分中，有几十种都具有抗癌作用。 白酒含有醇类、酯类等芳香类化合物，香味特别醇厚。 绍兴旧俗，每当有户人家生下小孩时，家人就会用糯米酿一坛酒，埋藏地下。如果生的是男婴，则称此酒为“状元红”，如果生的是女婴，则命名为“女儿红”。等到孩子成人，到大婚之日，才将酒从地底下挖出来招待宾客。这样一来，岂不是全民酿酒？如此大规模的持续性的酿酒行为，不断地提升着绍兴黄酒的制作工艺，造就了绍兴黄酒今天无法逾越的地位。海鲜、河鲜是该地区的重要食材，黄酒中的羰基化合物能和水产类食材中引起腥味的胺类物质发生反应，消除腥味。 烹调肉类时，先用黄酒进行腌渍再烹调，这时酒中的乙醇与水解的油脂脂肪发生酯化反应，生成酯类芳香物，产生复合香味。黄酒自身所含的醛类和氨基酸在烹调时也会产生有色香味物质。烹制菜肴时添加黄酒的最好时间是在锅中温度达到最高时，在高温下，黄酒的各种成分能够充分地发生化学反应，最大限度地发挥其去腥解腻，增香着色的作用。 啤酒是人类最古老的酒类之一，它是用大麦花、酒花、水为主要原料，通过酵母发酵酿成的包含二氧化碳的低酒精饮料。公元前 6000年左右的巴比伦人就用黏土板记载了啤酒的做法，公元前 3000年左右啤酒进入了欧洲，直到 20世纪初俄国人在哈尔滨建立啤酒厂之后，中国才开始出产啤酒， 姜酒是粤菜中的一味常用调味料，是以药食兼用的姜作为原料制成的一种保健酒。姜酒含有生姜的挥发油成分，因此尽管酒精度低，酒香却十分醇厚，很适合做调味酒。在烹制水产类食材，或者野味时使用姜酒，能够去除腥味、异味，而且还不用担心食材原有的鲜味遭到破坏。 葡萄酒起源于欧洲，与中国的渊源却可追溯到两千年前。汉代张骞出使西域不仅带回了葡萄，也带回了葡萄酒的酿造工艺和技术。 葡萄酒本身就是西餐中的料酒之一，用葡萄酒烹调不仅能增加菜肴的香味，而且能保持海鲜或肉类食材的水分。将食材用葡萄酒浸泡可以使酒香渗透到食材中，同时酒的酸度可以软化食材。在烹制白肉、鱼类、贝类时一般选用白葡萄酒，而红葡萄酒则常被用于牛肉、猪肉等红肉的制作。 开启后放置一段时间的葡萄酒会变酸，这时会影响菜的口味。这是因为酒精发生氧化反应，变成了醋。 2. 调料的运用咸味的运用 在对原料进行腌渍时，应根据不同原料的特性，掌握好腌渍的时间及用盐量。 碘是一种不稳定的化学物质，具有挥发性，特别是遇热极易挥发。正确的方法应该是在菜即将出锅时再加入碘盐，这样效果好，能使人体充分吸收利用碘盐中的碘，达到补碘的目的。 烹调菜肴时，两次加盐好：烹调菜肴时，调味加盐应分两次放入。其方法是：主料与油入锅后，先放进盐量的 1/ 3；菜肴快熟时，再将剩余的 2/ 3的盐放入。这样调味效果更佳。因为盐溶液渗透压高，如果烹制开始就放入所用盐量，则会使肉类食品中蛋白质过早变硬，不易熟透，菜肴口感生涩；使蔬菜原料过早出汤，失去脆嫩，并损失部分营养成分。 菜肴炸与汆，用盐不一般：炸制菜肴应该先腌再炸，这样容易入味，但腌制时放盐要少，以占整个口味八成左右为好。汆制菜肴与炸制菜肴相反，盐应相对多放一些，因为盐是水溶性物质，原料在水中汆制时，盐会有一部分溶于水中。 炖煮菜清淡，盐量把握好：炖煮菜肴的原料以鲜活为主，经过长时间炖煮，汤中溶解了大量的氨基酸，特别是其中的谷氨酸，是鲜味的主要成分。谷氨酸与盐反应，生成谷氨酸钠，这是味精的主要成分，只要把盐放适量，与汤中谷氨酸充分反应，菜肴的鲜味就能最大限度地呈现出来。 四川泡菜好，用盐有讲究：最适合制作泡菜的用盐，是以自贡产生的井盐为最好，这种盐色泽白净，颗粒细小，杂质少，氯化钠含量在 95%以上（目前市场所售加碘的井盐并不适合做泡菜）。 根据油品种，定下盐时间：用动物油炒菜，应在放菜前下少许盐，这样能减少动物油中有机氯的残留量，待菜炒至合乎要求时再补加盐；用豆油或菜油炒菜，应先放菜后下盐，可以减少蔬菜中营养成分的流失；用花生油炒菜，应先下少量的盐，因为花生油中可能含有黄曲霉菌，盐中的碘化物能除掉这种有害物质。 豆豉酱入菜，掌握四要点：1.应剁成末或搅成泥。 2.应先用热底油炒一下，再下入其他调料，成菜豉香味才浓。 3.炒制时切勿火大。 4.最好将其先制成“豆豉半成品”，以便于菜肴制作中调味。 炒菜用酱油，不宜过早放：炒菜放酱油的最佳时机是在菜接近炒好、将出锅之前，这时放酱油，才能起到调味、调色作用，又能保持酱油的营养价值及鲜美滋味。 甜的运用 偏甜菜调味，糖盐有顺序：制作口味偏甜的菜肴时，应先加糖后加盐。 甜面酱入菜，先用热油炒：甜面酱是居家常用调味料，使用时不宜直接放入菜肴中，而应先用热底油炒熟炒透，再与原料合炒成菜，酱香味才浓郁。 酸 烹鱼加些醋，除腥又提香：在烹调前，通过调味料食醋中醋酸与腥臭气味的碱性物质发生中和作用生成盐，而使腥臭气消失殆尽，突出鱼肉的香气。醋酸与醇类发生酯化反应，生成具有挥发性的酯类等香味。 加醋烧猪蹄，营养风味好：在烧猪蹄时，稍微加一些醋一起烹调，就可使猪蹄中的蛋白质易于被人体消化、吸收和利用。因为猪蹄中主要含有的胶原蛋白，在加酸的热水中易从猪蹄上分解出来，并使猪蹄骨细胞中的胶质分解出磷和钙，使营养价值增加。此外，加一些醋，还可消除猪蹄中部分重油腻味，增加猪蹄的风味。因此，烧猪蹄加一些醋，既营养，风味又好。 牛肉烧和煮，点醋速熟烂：牛肉中胶原蛋白质在酸性环境中加热，可以加速分解成可溶性、柔软的明胶。 制作红烧肉，加醋效果好：醋可以促进蛋白质在加热的条件下更多地水解成氨基酸，使烧出的肉味道鲜香、醇厚，同时使肉中的蛋白质更易被人体消化和吸收。醋与料酒中的乙醇、肉中脂肪水解出的脂肪酸结合成脂肪酸酯和乙酸乙酯。 辣椒有辣味，加醋可消减：可在烹饪新鲜辣椒菜时放点醋，这样辣味就不会那么重了。原因是辣椒中的辣味是由辣椒碱产生的，而醋的主要成分是醋酸，故放醋可中和辣椒中的部分辣椒碱，去除大部分辣味。此外，放醋还可防止菜中维生素 C的损失，有利于人体消化和吸收。 突出酸味菜，醋分两次放：突出酸味菜，醋分两次放，易挥发。 挤点柠檬汁，烤肉味道奇：用明火烤肉时，淋点柠檬汁，不仅能解除部分致癌物质的毒性。还能帮助肉类释放其自身的香味，使烤出的肉更鲜嫩味美。 酸性菜肴中，不宜加味精：味精必须在适宜的盐溶液中才能发挥鲜味作用。一般情况下，味精应在中性或弱酸性条件下使用，不宜放在过碱或过酸的汤菜中。 辣 青花椒调馅，不能直接用：用青花椒调肉馅，不能直接用青花椒粒去调制肉馅。因为这样不但不能压腥提香，反而会使肉馅沾上苦涩味。正确方法是：将青花椒用沸水泡一下，捞出花椒粒，取花椒水拌猪肉馅、羊肉馅、牛肉馅，用于水饺、馄饨等一类的小吃，效果特别佳。 青花椒入菜，加热忌过长：青花椒烹制热菜时，不宜加热过久。否则，菜肴会挥发出一种闷人的异味，且麻味也不纯正。 烹调羊狗肉，花椒应少放：烹调羊狗肉，花椒应少放花椒可除去各种肉类的腥臊臭气，并且促进唾液分泌，增进食欲，很适合烹调肉食。 鲜 烹菜用味精，投放有时间：味精有不耐高温的特性，长时间的高温加热，会使谷氨酸钠变性。根据这一特点，味精适宜在 100 ℃左右、短时间加热条件下使用。若在高于 120 ℃的温度下或在 100 ℃左右长时间加热，味精中的谷氨酸钠就会变成焦谷氨酸钠。焦谷氨酸钠既没有鲜味，还具有一定的毒性。因此放味精的最佳时间是在菜肴临出锅之前。 味精提鲜度，论菜而施加： 炒鸡蛋时也不宜放味精，因鸡蛋本身含有与味精相同的成分——谷氨酸。 凉菜加味精，先用水化开：味精加入凉菜以前，用少量 70 ℃ ～ 90 ℃热水溶解以后，再拌入菜肴内，既均匀，鲜味又突出，效果比较好。 蚝油味咸鲜，混用有禁忌：蚝油不能与醋以及辛辣调料共用，因为这些调料均会掩盖蚝油的鲜味， 蚝油不能与醋以及辛辣调料共用，因为这些调料均会掩盖蚝油的鲜味， 蚝油来调味，久煮失鲜味：蚝油若不加热调味，味道将逊色些。但加热时间不能过长，以免失去鲜味， 肉料腌味时，加入蚝油好：肉料腌味时，加入蚝油好 冻鱼入肴馔，返鲜有妙招 ：1.在汤汁中放一些鲜牛奶，这样鱼的味道可以接近鲜鱼。 2.将冻鱼放在含有少量盐的冷水中化冻，使鱼肉蛋白质遇盐后慢慢凝固。 3.烧鱼时，加入少许米醋或黄酒。这样烧出的鱼，既肉质鲜嫩，也无腥味。 1.在汤汁中放一些鲜牛奶，这样鱼的味道可以接近鲜鱼。 2.将冻鱼放在含有少量盐的冷水中化冻，使鱼肉蛋白质遇盐后慢慢凝固。 3.烧鱼时，加入少许米醋或黄酒。这样烧出的鱼，既肉质鲜嫩，也无腥味。 香 酒类品种多，做菜数黄酒黄酒作为理想的调味品而不用其他酒呢？因为黄酒中含乙醇 15%，比白酒少，比啤酒多，含糖分和总酸度比白酒高，在烹调时醇和酸生成酯类为菜肴增加芳香，同时乙醇能去除腥味和肉的荤味。 烹调用到酒，掌握好时间烹制时加入料酒的最佳时间是锅中温度最高的时候。有的菜肴需要突出酒香味，如贵妃鸡翅、汾酒牛肉等菜肴，就应当在主料成熟后调入酒，以保证酒香味不会过多地挥发。 原料异味重，先烹酒后醋 调制香料水，沸水浸焖好调制香料水，沸水浸焖好香料水是用桂皮、花椒、八角茴香等多种天然香料调制而成的， 烹制红烧鱼，香料热油炒​烹制红烧鱼时，所用的八角和各种酱料等，一定要入热油锅中炒香出色后，再掺入适量清水烧沸，放入处理过的鱼烧制。 葱姜蒜炝锅，油温要适中姜炝锅时，油温应掌握在三成热左右；而葱、蒜炝锅时，油温需掌握在两成热左右；如用葱、姜、蒜炝锅，将油温升到 50 ℃，先下入葱、蒜炒片刻，再下入姜，至葱、姜、蒜呈浅金黄色和老黄色时，接着下入主料、配料， 葱姜蒜炝锅，用火不能小时间过长香味会跑掉。 参考书籍:《调味圣经》 牛国平, 湖南科学技术出版社]]></content>
      <categories>
        <category>books</category>
      </categories>
      <tags>
        <tag>cook</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Special for Qixi]]></title>
    <url>%2F2019%2F08%2F07%2Fqixi%2F</url>
    <content type="text"><![CDATA[七夕特辑 帝国历2019年 “罗星，你真的要走了吗？你应该知道这条星路有多么艰险，以至于从来没有人去探索。” “这次在天鹰alpha附近发现了极有可能适宜人类居住的行星，为了给帝国的版图增加新的疆域，我必须离开。对不起，小曦！” “呵呵，帝国。你的帝国早就已经开始腐朽。你终究还是走上了和那个男人一样的路，我早该想到你们本就是一类人，口口声声的帝国，不过是为了自己的荣誉罢了。可笑的是那个男人寻找到的天琴alpha，如今却成了反抗帝国的种子。” “你的父亲是帝国光荣的战士，他的功勋不容质疑。” “是啊！帝国帝国，你们的眼中只有帝国。你可以像那个男人一样，但我却不会和我的母亲一样让自己沉睡在冬眠仓里等着那个人回来。我已经决定了，我会前往天琴alpha的反叛军基地与你遥遥相对。我会在那里见证着你的帝国慢慢的死去。若你想要阻止我，现在就杀我了吧！为了你的帝国！” “你走吧。”罗星轻轻道，似乎并不在意。 陈曦头也不回的离开了，她已经彻底的失望了。而罗星只是静静地站立着，望着她消失在视线中，突然默念了一句：“好好保重。” 联邦历1年 “今日我们要宣布星际联邦正式成立。我们要感谢伟大的先驱们，他们开拓了天琴alpha与天鹰alpha两大基地，以此对帝国形成合围，最终将腐朽的帝国与贵族们扫进了历史的尘埃里。在人类的母星地球时代曾有着牛郎和织女的传说，天鹰alpha和天琴alpha便是古老传说中的牛郎星和织女星。传说中，牛郎和织女相爱而不能相见，一年相会一次；而今天这两颗星终于被联邦永久地连在了一起。两颗微弱的星光，终究照亮了这片苍茫的星河。” 广场上，所有的人们都开始欢呼庆祝。而陈曦就这么静静站着，凝望着那颗遥远的牵牛星。突然后面传来一句熟悉的声音：“曦，我回来了。” 纯属瞎写，切莫深究。 欲知详情，自行脑补。 ​ weiuniverse]]></content>
      <categories>
        <category>science fictions</category>
      </categories>
      <tags>
        <tag>festivals</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[西方哲学史 读后随想]]></title>
    <url>%2F2019%2F07%2F24%2Fhistory_of_west_philo%2F</url>
    <content type="text"><![CDATA[西方哲学史 —— 罗素 阅读笔记限于笔力和水平有限，未能全部完成，许多细节之处也未详尽。 1）简介​ 书籍简介 西方哲学史是英国哲学家、逻辑学家、数学家伯特兰•罗素的代表作之一，也是罗素获诺贝尔文学奖的因素之一。本书主要阐述了从西方哲学萌芽的古希腊哲学一直到二十世纪早期期西方哲学的发展历程，描述了近三千年西方哲学与西方社会的演变，提到了从古希腊泰勒斯到近代的马克思康德杜威等数十位甚至上百位哲学家。 作者简介： 罗素出生于1872年，当时大英帝国正值巅峰，逝于1970年，此时英国经历过两次世界大战，其帝国已经没落[2]。他是英国哲学家，数学家，逻辑学家，致力于哲学的大众化，普及化[2][3]. 罗素与中国： 罗素曾于1920年访问中国，在北京讲学。在长沙时期，青年毛泽东曾担任记录员。 1960年，毛泽东和周恩来也曾向罗素发出邀请，但限于身体未能成行，但送了一本西方哲学史给毛泽东。 罗素于1922年曾发文 中国问题。 半个世纪以来，由于罗素个人思想的高超，使他一直成为全球瞩目与争论的中心。在人类知识和数理方面，他的研究成果可以与牛顿在力学上的成就相媲美。但并不是由于他在这方面的成就而获得诺贝尔文学奖，而是因为他能够把一般性的哲学思想成功地介绍给人们，他这样做，是对哲学家始终保持兴趣的最成功的范例。 ——1950年罗素获得诺贝尔文学奖的颁奖词 2） 读后感要感谢罗素老哥，让我这样一个对哲学几乎一无所知，对西方历史也是一知半解的人在短短的十几个小时里窥见了一个如此新奇有趣的世界。 罗素先生笔下的这本西方哲学史或许并不能算是一本标准的史学。书中掺杂着大量的自己的观点，让人觉得仿佛是一个好友在跟你吃饭的时候纵论古今，数风流人物。从泰勒斯到毕达哥拉斯，再到古希腊三杰（苏格拉底，柏拉图，亚里士多德），古希腊的辉煌令我惊叹。穿过中世纪的黑暗，政治的统一与思想的单一化，教皇与帝王的争斗也曾一度让我觉得有些苦闷与枯燥，来到文艺复兴与近现代，新的有趣的思想又不断涌现，笛卡尔康德马克思一个个熟悉的人物在我面前不断鲜活。 不知是刻意为之，还是因为骨子里已经刻上了怀疑主义的烙印，罗素先生似乎更侧重于对历史的批判，他固然承认这些哲学家在那个历史时期的重要作用，但也不断的在指出他们的局限性以及错误。他似乎是想以这种方式让这些哲学家变得鲜活起来，把哲学推下神坛，与普通人离得更近，也似乎是想以这种方式来告诉我们要敢于去怀疑先人。 识人如读书，读书亦是在识人。感谢这本西方哲学史让我认识了一个有趣博学充满怀疑精神对这个世界怀有着善意的罗素，让我觉得自己仿佛真的有一个这样朋友在跟我纵论古今，谈笑风生。 古希腊的辉煌古希腊可以说是西方哲学史最璀璨辉煌的时刻，我以为绝大多数的现代西方哲学都还留存着古希腊哲学的影子，甚至可以说西方哲学史几乎就是古希腊哲学的流传和发展演变的历史。与之相对应的是中国的诸子百家时期。在这段璀璨的时光里，诞生了： 哲学的祖师泰勒斯Thales； 万物皆数的毕达哥拉斯Pythagoras，江湖上至今仍流传着他的思想； “永远不能踏进同一条河流”的赫拉克利特Heraclitus； 原子论德谟克利特Democritus； 古希腊三杰不怕死但怕老婆的苏格拉底，因为他认为死后的世界更加美好，更加适合搞科研，不，搞哲学； 以及他的学生与爱情难解难分的柏拉图，也可以说是最早倡导同性恋的人了：柏拉图式爱情认为精神上的爱情才是纯洁的，后半句是同性之间更容易产生精神恋爱； 以及他学生的学生活在初中物理课本里的男人亚里士多德，甚至可能不至于物理课本。亚里士多德这个人爱好广泛，又喜欢写书，著作包括了物理学、形而上学、诗歌（包括戏剧）、音乐、生物学、经济学、动物学、逻辑学、政治、政府、以及伦理学，几乎影响了牛顿之前的漫长的时代。然而晚节不保，终究是被牛逼顿推下神坛，从此被钉在了初中物理课本上。当然，这人的思想在当时可以说是极为超前，为科学的萌芽与发展也奠定了一定的基础，甚至于他的哲学还影响至今。 古希腊的辉煌可以说是非常传奇的，至今仍被许多学者研究着。当然我们现代的思想终究是在不断进步，以现在的观点去看，那个时候的哲学家的一些思想看起来甚至有点幼稚，但是 那个自由激烈的思想不断迸发，天才而富有活力的大脑层出不穷依旧令今天的我们神往之。 太空城邦时代我以为古希腊的城邦制度或许是思想迸发的原因之一。正如春秋战国时期的中国也一样。制度与文化的多样化带来思想的多样化。 设想一下，若是未来，人类进入太空时代，受制于人类在宇宙中的航行速度，是否人类文明又会重回城邦时代，或者叫做星际联邦时代。不同的星球城邦之间限于地理位置以及地理隔离等因素，渐渐演变不同的制度与思想。是否又会重现古希腊的辉煌，一个百家争鸣的时代又是否会再度到来。 又有没有一种可能：进入太空时代之后，人类渐渐的发现物理规律似乎难以解释太空中的某些现象，人类再次被自然的力量所击倒，古老的形而上学的哲学再度复苏，宗教与神学的力量又再度兴起。 而当人类再次将星空之路打通，将不同的星球城邦文明再度连接，是否帝国又会重启。当贸易开始繁荣，商人开始在星空之间辗转，是否资本的力量又会再度归来。但是已经经历了一遍这样的历史的人类，面临同样的局面是否又会做出不一样的选择呢。未来的一万年，太空时代的人类文明，哲学思想与社会又是会怎样的发展。 事实上，我认为阿西莫夫的《基地》正是再以太空帝国的衰落为背景，写一个宗教兴起，资本兴起的故事。他设想的是两万年以后人类文明的样子。那么在未来这两万年，从地球到太空帝国时代，这段时间人类文明又会是什么样子呢，是否会是真的有一个太空城邦百家争鸣的时代。 文明与野蛮：文明与野蛮在于眼光更长远。但眼光长远是理性的，也是苦闷的，因为美好永远在将来。文明想要消灭冲动不仅要靠理性，还要靠法律道德宗教。所以说每个人都是文明与野蛮的合体，我们有时可以为伊消得人憔悴，蓦然回首，有时也可以人生得意须进欢，今朝有酒今朝醉。我们除了通过理性压抑自己的欲望，也通过精神能量来扭转当下的苦闷，把此刻的努力与未来的美好结合起来，把此刻的苦难当做一种富有乐趣的挑战。感觉文明与个人在这一方面还是相对统一的，以一个文明的发展来折射到个人的命运，似乎也有一些乐趣。 中世纪黑暗时代可以看一下阿西莫夫的《基地》，以未来的太空时代对比历史。待续 近现代：哲学与科学：待续 浪漫主义浪漫主义的观点之所以能打动人，是人的本性与社会环境共同决定的。为了生存，人不得不群居，但是人的本性一直是孤独的；于是人们用宗教和道德来约束自己，面对现实。为了将来不得不违背人的本性，这让人感到沮丧。所以当浪漫主义将大家的热情激发出来之后，人们便对宗教和道德的约束难以忍受了。人们纷纷摆脱约束和束缚，享受人性解放后的自由；他们当时没有注意到后来可能遭遇到的不幸。人不是离群寡居的动物，只要社会生活还在继续着，伦理的最高原则就不可能是自我实现。 以下为浪漫主义鼻祖卢梭的个人经历（节选自西方哲学史，未经查证，全当八卦传奇故事） \1. 之后，她的家人发现卢梭有一个原本是夫人的饰扣。那是他偷来的，但他拒不承认，只是说是某个他喜欢的女仆送给他的。大家听信他的话，处罚了他所说的那个女仆。\2. 之后，德·华伦夫人接济了他。德·华伦夫人也是一名新教改宗者，她是一名美丽的贵妇，因为在宗教上的贡献，她每年都从萨瓦王那里领取年金。卢梭在她家中住了将近十年，德·华伦夫人成了他的情妇，同时还是他的义母。\3. 一位有钱的贵妇人上了他的当，两人还闹出了一次桃色事件。 \4. 1743年，在一位贵妇的帮助下，他成了当时法国驻威尼斯大使德·蒙泰古伯爵的秘书。 \5. 两年后他开始同旅馆中的佣人黛蕾丝·勒·瓦色同居，并一直生活在一起，直到死去；两个人一共育有五个儿女。\6. 他认为科学是美德的反义词，它扩大了事物丑陋的一面。例如，天文学源自迷信的占星术；雄辩术源自政治野心的需要；物理学源自无聊；伦理学则源自人类对自己的自卑认识。 “虚无主义”，不可知论与怀疑主义（此处的虚无主义是我自己的理解）“初因”论证认为，有限的事都是有原因的，原因也有原因，逐个类推，但这些原因不会无穷无尽，至少第一个原因是没有原因的，这就是神。——莱布尼兹 这世上的大多事物，当我们顺着因果线往上游追溯的时候，往往会发现一切都失去了指向。有人将这种上游定义为上帝或者神，便成了宗教。也有人将这种上游定义为四十二，便成了科幻。我将这种毫无指向的因果上游定义为虚无，只在下游去讨论和思考，便成了我的虚无主义。 当我把一切事物的终极因果定义为虚无的时候，也就间接地承认因果不可追溯到起点，也就是说我们无法得知宇宙的终极秘密，以及一切事物存在的意义，这就与不可知论有些重合。 而怀疑主义，我认为极致的怀疑主义怀疑一切，自然就不可知了。当然大多数时候，怀疑主义更多的指的是一种对知识的怀疑。罗素先生便是一个怀疑主义者，他说：“我不会为自己的信仰献身，因为我可能是错的。”我以为也是如此，一个真正的怀疑主义者，要从怀疑自身开始，但并不等于否定自我的意义，而是把怀疑主义已经融入自我的一部分，让自我在怀疑中不断进步，升华自我的意义。 我也并不认为虚无主义和不可知论是消极的，反而正是因为没有一个终极的意义存在，所以我们才有无限的未知去探索；正因为没有终点，我们才能一直在路上。对于人类而言，倘若有一天真的知晓了这宇宙中的一切奥秘，失去了一切前进的方向，那该是怎样的悲凉。当然或许，那个时候的人已不再是人，而是神了，而神的想法不是我所能揣度的。我所认为的虚无主义与不可知论，否定了因果起点的存在，但并不否认整个因果链的存在意义，所以我们依旧可以在因果链的下游去定义我们的存在我们人生价值。 写在最后：罗素先生知识渊博，令人神往。遗憾之处是由于时间和个人水平所限，未能认真阅读原著。只能通过译者的口来与罗素先生交流。仓促读完，许多细节之处也未能一一查证思考，权当脑子里的记忆了。毕竟没有认真思考和怀疑过的东西是很难当作自己的知识的。但凡是思考过的一些观点，都给了我不少的启发。作为一个深刻的怀疑主义者，虽未全盘接受，但也是受益匪浅。许多未尽之处都要等到闲暇之时，再细细品读了。 3） 经典摘抄希腊文明的崛起 文明人与野蛮人的主要区别在于眼光更长远，尤其在农业兴起之后。为了冬天有粮吃，夏天就开始辛苦劳作，野蛮人是不会做这样的傻事的。当理性代替冲动的时候，才能证明是真正的文明人。打猎是即兴的、冲动的，而耕种需要等很久才能收获，是理性的。眼光长远是理性的，但也是苦闷的，因为美好永远在将来，当下永远有苦难。 毕达哥拉斯 毕达哥拉斯教义中认为人的身体不过是灵魂在世上寄托的地方，我们无以逃避，最好的净化灵魂的方式是献身无欲无求的科学事业，当一名旁观者和观察者。投身科学与沉思、观察催生毕达哥拉斯得出了数学知识。 亚里士多德的形而上学 他认为东西肯定有界限，这种界限构成了东西的形式。如大理石雕像的形式是它的形状，如果不把大理石从山中开采出来经过雕刻，把它同山中乱石区分开来，那它的实质就是石头，而不是大理石雕像。是形式让质料有了实质，是事物的界限让它有了形式。 弗兰西斯·培根 实用性是培根哲学的基础，表现为利用科学技术，使人类发现或发明能够控制自然的力量。培根信仰传统宗教，主张哲学和神学应该分离，虽然认为从理性出发，也能证明确实存在神灵，但在他看来，除此之外的神学都是凭借启示认识的。 笛卡尔 为了使他的哲学思想获得牢固的基础，笛卡尔决心怀疑他能够怀疑的一切事物。他首先从怀疑各种感觉开始。他说，在我把所有事物都想象成虚幻时，这时的“我”一定是存在的某种东西；我明白“我思故我在”是可靠和准确的真理，就连怀疑论者的任何狂想都不能否定它，因此，我可以百分之百地断言，这是我追求的哲学里的第一条真理。这是笛卡尔认识论的核心，包含了他的哲学思想中最重要的内容。 莱布尼茨 宇宙论论证是“初因”论证的一种。“初因”论证认为，有限的事都是有原因的，原因也有原因，逐个类推，但这些原因不会无穷无尽，至少第一个原因是没有原因的，这就是神。莱布尼茨说，所有的个别事物是偶然发生的，可能它本来不存在，整个宇宙也可能是这样。按照莱布尼茨的说法，整个宇宙得有个充足的理由，它就是神。 分析哲学 与当时的哲学各学派相比，现代分析经验主义有一个有利的条件，即：不必试图一次创造出关于全宇宙的一整套理论，而是可以逐个地解决问题。 引用： [1] 《西方哲学史》 罗素 [2] 维基百科-罗素https://zh.wikipedia.org/wiki/%E4%BC%AF%E7%89%B9%E5%85%B0%C2%B7%E7%BD%97%E7%B4%A0 [3]古腾堡计划中伯特兰·罗素的作品]]></content>
      <categories>
        <category>books</category>
      </categories>
      <tags>
        <tag>philosophy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[dl_tips]]></title>
    <url>%2F2019%2F01%2F28%2Fdl-tips%2F</url>
    <content type="text"><![CDATA[MathJax.Hub.Config({ tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]} }); 1.数据 数据预处理 数据集尽可能大 删除或填补所有具有损坏数据的训练样本 数据增强: 创建新样本 2. 适当的激活函数 通常使用Relu函数， Sigmoid函数的缺陷：尾部饱和梯度消失 3. 网络结构 Resnet，通常选用默认结构，或进行微调 使用比最佳隐含单元数多的数量通常是安全的，因为正则化方法一定程度上都可以处理多余的单元 隐含层只需不断增加，直到性能不在提升。层数越多表达能力越强，但过多时容易过拟合。 4. 权重初始化 使用小随机数初始化权重，通常使用框架推荐设置,如Xavier, He。 5. 超参数微调 网格搜索，效率较低 靠经验调整 随机搜索 6. 优化方法 Adam通常使用默认值 Adam比SGD好调且收敛速度快，但SGD + momentum 通常效果更好 7. batchsize batchsize 过大影响梯度下降的随机性，过小则随机性过大，收敛不稳定。 通常100左右，可以以一定倍数增加或者减少 使用batch normalization 8.可视化 使用tensorboard or tensorboardX 可视化训练过程，观察过拟合情况，选取最优模型。]]></content>
      <categories>
        <category>learning_notes</category>
      </categories>
      <tags>
        <tag>deep_learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[目标检测论文重读 - RCNN]]></title>
    <url>%2F2018%2F03%2F28%2FRCNN%2F</url>
    <content type="text"><![CDATA[简介近期发现之前很多目标检测的论文都只是粗略的读了一遍，只把关键点过了一遍，现打算把论文再拿出来多读几遍，把论文中的一些思路和细节总结一遍。 首先从RCNN说起吧。 主要步骤RCNN算法分为4个步骤 候选区域生成： 一张图像生成1K~2K个候选区域 （采用Selective Search 方法）特征提取： 对每个候选区域，使用深度卷积网络提取特征 （CNN）类别判断： 特征送入每一类的SVM 分类器，判别是否属于该类位置精修： 使用回归器精细修正候选框位置 要点1. Region Proposal 候选区位置选择论文中提到了几种经典的方法， objectness, selective search, category-independent object proposals, constrained parametric min-cuts(CPMC), multi-scale combinatorial group 。最后文章中采用了 selective search 的方法。由于现在这些方法基本都被 Region Proposal Network 取代了，所以暂时不做进一步调研。 2. CNN 特征提取在这篇文章之前也有一些文章尝试使用 CNN 来提取特征用于检测任务， 如overfeat. 其他细节和分析1. Visualizing learned features第一层： capture oriented edges and opponent layers.使用一个简单的非参数方法，将10 million个proposals根据单个单元的activations 进行排序，并采用非极大抑制，然后将期中top 16的proposal和acitvation 可视化出来进行观察。 结论: 网络学习到的表达是 class-tuned features togather with a distributed represenataion of shape, texture, color and material properties. 2. Ablation(消除) studiesperformance layer by layer, without fine-tuning直接使用pretrained model 在 PASCAL VOC 上提特征，并用于SVM的训练。 发现： 直接使用卷积层的特征比加上最后两层连接层的效果要好，尽管卷积层只占了6%的参数。这 告诉我们卷积层学到的图像特征更加general. performance layer by layer with fine-tuning 提升很大 fc6,7 层比pool5变化大，说明pool features 更加general. 进行fine-tuning之后最大的提升是来自于顶层，而非底层的特征提取器。 Ref: Rich feature hierarchies for accurate object detection adn semantic segmantaion. RCNN- 将CNN引入目标检测的开山之作。晓雷]]></content>
      <categories>
        <category>deeplearning</category>
        <category>computer vision</category>
      </categories>
      <tags>
        <tag>Object_Detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[face_times_A_website_of_face]]></title>
    <url>%2F2017%2F12%2F02%2Fface-times-A-website-of-face%2F</url>
    <content type="text"><![CDATA[MathJax.Hub.Config({ tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]} }); IntroductionWe design a website to do face detection, face landmark localization, face swap, face age and gender recognition. Face detectionHarr cascadeAt first, we tried harr cascade with opencv to do face detection. However, this method cannot achieve a comparable performance. DlibThen, we switched to Dlib. And use the dlib.get_the_frontal_face() as detector. Due to the limited time, we cannot choose better method like SSD, Mask-RCNN. SSDWe plan to use Single Shot Detection method to imporve the performance. Face AlignmentWe use dlib to implement this function. Dlib implement the paper One Millisecond Face Alignment with an Ensemble of Regression Trees]]></content>
      <categories>
        <category>computer vision</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Estimating Accuracy from Unlabeled Data A Probabilistic Logic Approach]]></title>
    <url>%2F2017%2F11%2F15%2Festi-accur-psl%2F</url>
    <content type="text"><![CDATA[MathJax.Hub.Config({ tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]} }); IntroductionBackgroundEstimating the accuracy of classifiers is central to machine learning and many other ﬁelds. Most existing approaches to eastimating accuracy are supervised, meaning that a set of labeled examples is required for the Estimation. However, being able to estimate the accuracies of classifiers using only Unlabeled data is very important for many applications. Futhermore, tasks which involve making several predictions which are tied together by logical constraints are abundant in machine learning. IntuitionMutual ExclusionIf domains d1 and d2 are mutually exclusive,then $f^{d_1} =1$ implies that $f^{d_2} =0$.For example, in the NELL setting, if a NP belongs to the “city” category,then it cannot also belong to the “animal” category. Subsumption For example, in the NELL setting, if a NP belongs to the “cat” category, then it must also belong to the “animal” category. Proposed MethedDefine a set of Logic Rules(i) defining a set of logic rules for modeling the logical constraints between the $f^d$ and the $\hat{f}^d_j$ ,in terms of the error rates $e^d_j$ and the known logical constraints. Probabilistic LogicIn classical logic, we have a set of predicates (e.g., mammal(x) indicating whether x is a mammal, where x is a variable) and a set of rules deﬁned in terms of these predicates. These ground predicates and rules take boolean values. In probabilistic logic, we are instead interested in inferring the probabilities of these ground predicates and rules being true, given a set of observed ground predicates and rules.Furthermore, the truth values of ground predicates and rules may be continuous and lie in the interval [0,1], instead of being boolean,representing the probability that the corresponding ground predicate or rule is true. Model Function Approximation Outputs: $\hat{f}^d_j(X),j=1,…,N^d, inputs X\in \chi$ Target Function Outputs: $f^d(X),inputs X\in \chi$ Function Approximation Error Rates: $e^d_j,j=1,…,N^d$ Ensemble Rules$$\hat{f}^d_j(X)\land \neg e^d_j \rightarrow f^d(X),\neg \hat{f}^d_j(X)\land \neg e^d_j \rightarrow \neg f^d(X)$$$$\hat{f}^d_j(X)\land e^d_j \rightarrow\neg f^d(X),\neg \hat{f}^d_j(X)\land e^d_j \rightarrow f^d(X)$$ ConstraintsMutual Exclusion Rule$$ME(d_1,d_2)\land \hat{f}^{d_1}_j(X) \land f^{d_2}(X) \rightarrow e^{d_1}_j,for d_1\neq d_2=1,…,D,j=1,…,N^{d_1},and X\in \chi $$ Subsumption Rule$$SUB(d_1,d_2)\land \neg\hat{f}^{d_1}_j(X) \land f^{d_2}(X) \rightarrow e^{d_1}_j,for d_1=d_2=1,…,D,j=1,…,N^{d_1},and X\in \chi$$ Perform probabilistic inference(ii) performing probabilistic inference using these rules as priors,in order to obtain the most likely values of the $e^d_j$ and the $f^d$,which are not observed. Probabilistic Soft Logic (PSL)Define the terms: The unobserved ground predicate values: $\boldsymbol{Y}={Y_1,…,Y_m},Domain\ \boldsymbol{D}=[0,1]^m$. Observed ground predicate values: $\boldsymbol{X}={X_1,…,X_m},Domain\ \boldsymbol{D}=[0,1]^n$. Continuous potential functions: $\phi={\phi_1,…,\phi_k}$,$\phi_j(\boldsymbol{X},\ \boldsymbol{Y})=(max{ \mathcal{l}_j(\boldsymbol{X},\boldsymbol{Y}),0 } )^{p_j}$, $\mathcal{l}_j$ is a linear functions of X and Y,$p_j\in{1,2}$. Free parameters $\lambda={\lambda_1,…,\lambda_k}$ Define HL-MRF Density:$$f(\boldsymbol{Y})=\frac{1}{Z}exp(-\Sigma^k_{j=1}\lambda_j\phi_j(\boldsymbol{X},\boldsymbol{Y}))$$ Define logical operator$$P\land Q \triangleq max{P+Q-1,0},\quad P\lor Q \triangleq min{P+Q-1,0}$$$$\neg P \triangleq 1-P,\quad P\rightarrow Q \triangleq min{1-P+Q,0} $$ Grounding Solving the optimization problem Experiments]]></content>
      <categories>
        <category>paper_read</category>
      </categories>
      <tags>
        <tag>logic</tag>
        <tag>Unlabeled_Data</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[progressive_gan]]></title>
    <url>%2F2017%2F11%2F07%2Fprogressive-gan%2F</url>
    <content type="text"><![CDATA[ContributionsPrimary contribution: Propose a training methodology for GANs where start with low-resolution images, and then progressively increase the resolution by adding layers to the networks. Others: Increasing variation using minibatch standard deviation A new normalization in G and D Propose sliced Wasserstein distance (SWD) to estimate the statistical similarity. progressive training Training frame Transition from 16x16 to 32x32 Benefits of progressive TrainingMore Stable: Early on, the generation of smaller images is substantially more stable because there is less class information and fewer modes. By increasing the resolution little by little we are continuously asking a much simpler question compared to the end goal of discovering a mapping from latent vectors. The reduced training time: With progressively growing GANs most of the iterations are done at lower resolutions, and comparable result quality is often obtained up to 2–6 times faster, depending on the ﬁnal output resolution. sliced Wasserstein distanceMotivation The previous methods like MS-SSIM do not directly assess image quality in terms of similarity to the training set. Therefore, they propose a new metric sliced Wasserstein distance which will measure the distance between training set and generated samples. Experiments Convergence and Training speed ExperimentsCeleb High quality lsun CIFAR10]]></content>
      <categories>
        <category>deeplearning</category>
        <category>paper_read</category>
      </categories>
      <tags>
        <tag>GAN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tree-RL for object localization]]></title>
    <url>%2F2017%2F11%2F01%2FTree-RL-md%2F</url>
    <content type="text"><![CDATA[Motivation:current methods:Most of current object detection methods only use local image patch independently. However, the critical correlation cues among different proposals (e.g., relative spatial layouts or semantic correlations) are often ignored. Reinforcement LearningIn this paper, in order to exploit global interdependency among objects, they propose a Tree-RL approach that learns to localize multiple objects sequentially based on both the current observation and historical search paths. Reinforcement Learnin Recall To form a reinforcement learning frame, We need to define 3 components:ActionStateReward Definition of Actions There are 13 actions being divided into two group. The first group include 5 scaling actions, the second include 8 local translation actions. Definition of StateStates: concatenation of three components: 1.the feature vector of the current window, 2.the feature vector of the whole image 3.the history of taken actions. Definition of Reward Notes: 1.The first key reward stimulation +5 is given to those actions which cover any ground-truth objects with an IoU greater than 0.5 for the ﬁrst time. (The f is defined that when IoU(w,g)&gt;0.5, f=+1; otherwise f=-1.) 2.The second item means if the next window’s IoU(w’,g) is bigger than current’s IoU(w,g), r=+1, which means the window get closer to the ground truth; otherwise r=-1. Tree Structured Search This is the most different point with previous paper about rl for object detection.The Agent in this paper will select a best actions from two action group. The left is scaling action, the right is local translation action.And they will set the max level of the tree. For example, when setting the level equals to 4, there will be 15 proposals in the tree. And then these proposals will be put into a final classifier, which is similar to other proposal method. ExperimentsVisualization Recall Comparison to Other Object Proposal Algorithms Detection mAP Comparison to Faster R-CNN Here, they combined Tree_RL as proposal method with Fast RCNN to get the detection results.And they compare it with RPN + Fast RCNN and Faster RCNN method. We can see this paper score better than RPN but close to Faster R-CNN. But we should consider that Tree-RL only use a VGG-16 because of the limitation of computation.]]></content>
      <categories>
        <category>deeplearning</category>
        <category>Object_Detection</category>
        <category>paper_read</category>
      </categories>
      <tags>
        <tag>Object_Detection</tag>
        <tag>Reinforcement_Leearning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Conclusion for Generative Adversarial Networks]]></title>
    <url>%2F2017%2F10%2F28%2FGAN_conclusion%2F</url>
    <content type="text"><![CDATA[MathJax.Hub.Config({ tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]} }); Overview GAN DCGAN CGAN improv GAN LapGAN info-GAN WGAN 1. GANThe basic GAN model was proposed by Ian Goodfellow in 2014.This model include 2 Parts: generative model and discriminate model: Generative modelThe generative model intend to produce the sample close to real sample. Discriminate modelThe discriminator intend to improve the ability to judge whether the sample is real.]]></content>
      <categories>
        <category>deeplearning</category>
      </categories>
      <tags>
        <tag>GAN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[编程入门]]></title>
    <url>%2F2017%2F04%2F19%2Fprogramming%2F</url>
    <content type="text"><![CDATA[编程入门编程语言定义编程语言是用来定义计算机程序的形式语言。它是一种被标准化的交流技巧，用来向计算机发出指令。一种计算机语言让程序员能够准确地定义计算机所需要使用的数据，并精确地定义在不同情况下所应当采取的行动。 什么是编程编程语言是具有不同层次的，有机器语言，汇编语言，高级语言。要理解编程语言的层次，我们要从什么是编程说起。从上面对编程语言的定义来看，我们可以简单的把编程理解为人与机器交流的语言。可是人类和机器接受语言的形式是不一样的。 对于人类来说，语言是由词汇组成的语句段落，词汇（或者说构成词汇的字母）是人类语言的基本单元，词汇组合在一起从而传达信息。 对于机器而言，能够接受的语言形式则是0,1字符串(从硬件上来看就是电位的高低与否)，总之机器的语言便是0,1串，如0111101111对于机器而言可能代表了一定的指令信息。 那么编程所要做的事情就是把人类交流的语言和机器的语言联系起来。然而很不幸的事情是人类通用的语言逻辑过于复杂且充满了模糊性，这是目前的计算机技术所无法实现的。而编程语言正是为了解决这个问题所诞生的一种语言，我们通过设计特殊的语法，使得编程语言不在像我们日常使用的语言一样复杂且模糊，使得它更加接近计算机所能接受的逻辑。这是从目的上来理解编程语言。就是为了让计算机听的懂人话。 当然我们也可以从另一个角度来理解，机器语言由0，1串构成，对于人类来说简直难以忍受，试想如果我们都用01串来沟通交流，那是多么可怕的一件事情。然而事实上，最早的编程人员就是用机器语言来编程，那个时候的计算机还是使用晶体管(总之就是比较low)，他们会制作一堆插线孔，插上了就代表1，没有插上就代表0。所谓编程对他们来说就是插上成千上万根线，有一根线插错了也不行。这也就是为什么老一辈的人都认为电脑是一个特别难的东西，大概是当初留下了心理阴影。 正是因为机器语言这么麻烦，早年的程序员们自然就受不了了。为了方便自己方便后人，他们就发明了汇编语言。这个汇编语言说白了就是把01串用人话表示出来。汇编语言其实就是一个个简单的字母指令串，它包含了人们通常会让计算机做的指令，比如加(add)，减(minus)，还有变换地址等。从此程序员们就告别了使用插线板编程的苦逼的时代。人们开始通过汇编语言的指令串来指挥计算机，而机器内部又自有一套对应关系将这个指令集转变为机器语言让计算机读懂。 然而，汇编语言虽然相对于机器语言非常简单，但其实仍然特别麻烦，至少我是没有完全学会的。。。所以，伟大的程序员先驱们有创造了更加简单的语言，也就是LISP,Fortran,C,C++等高级语言。这些高级编程语言对于我们人类来说就更为友好了，至少比一大堆01串或者指令集友好的多吧。 但是这些高级语言们虽然对人类友好了，可是机器读不懂啊，所以就需要有一个翻译官来把高级语言先翻译成汇编语言。我们把这种翻译的过程叫做编译，把这个翻译官叫做编译器。(当然这个编译器本身其实也是一个程序了，以至于我曾经一直在想第一个编译器是用什么编译的，大概是某个大佬不辞辛劳的用汇编语言写了一个编译器吧，感谢大佬！当然对于大部分的coder来说是不需要知道编译器是怎么做的，用别人做好的就可以了。。) 什么是C语言首先C语言是一门高级编程语言，至于为什么高级当然是和汇编语言和机器语言来比了。 其次C语言是一门面向过程的语言，至于什么叫面向过程又不得不提面向对象吧，提到面向对象又不得不提C++,java等语言了。作为一个嫌麻烦的人还是先不解释吧，等后面提面向对象的时候再说。当然这个也可以自己先脑补一下，有助于脑洞开发。 编译器配置新手上路作为一个新手而言呢，其实编译器有两种选择。 第一种选择是自己配置 编辑器 + 编译器 的组合比较能够锻炼你的编程能力，当然也不好说，现在各种编程工具也是厉害，能用的好也是殊途同归。至于常用的编辑器有 Atom, Sublime text, notepad++,vim,emacs等，当然原则上你用记事本写也是可以的。对于编译器最常用的就是gcc了。具体使用方法就视编辑器不同了。 第二种选择是使用特定的IDE(集成开发环境，Integrated Development Environment)，通常来说使用简单，功能强大。作为一个windows用户，我自己最早用的是Visual Studio。功能强大，而且是微软自己家的，有庞大的技术支持。但是缺点就是其他系统用不了，而且比较大，功能太过齐全很多组件用不上。 Visual Studio的安装和使用参考教程 注意visual C++中包含了C语言。 学习资料菜鸟教程imooc教程 参考资料维基百科:C语言 维基百科:编程语言]]></content>
      <categories>
        <category>programming</category>
      </categories>
      <tags>
        <tag>programming</tag>
        <tag>introduction</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GAN overview]]></title>
    <url>%2F2017%2F03%2F13%2FGAN_overview%2F</url>
    <content type="text"><![CDATA[MathJax.Hub.Config({ tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]} }); 从生成模型到生成对抗网络1. 生成模型与判别模型主流的深度学习算法分为两种: 生成模型和判别模型。其中： (1) 生成模型:对联合分布 P(X,Y)进行建模。 X为输入，Y为输出或者隐含层。 直观来讲，我们认为 输入的数据X和输出的标签或者回归值Y合并的变量构成一个分布P(X,Y)。而我们所有的训练样本 (X,Y)均是由这个分布生成。对于生成模型的目的，就是学习到这样一个分布，也可以说是(X,Y)的联合分布。生成模型可以得到模型中任何一个变量的值。 (2) 判别模型：直接对 Y=f(X)或者 P(Y|X)建模。 而对于判别模型，我们认为输入数据 X 和输出值 Y构成函数关系，而判别模型的任务就是学习直接学习到这个函数关系，从而直接由输入X估计输出值Y。判别模型只能再给定输入的情况下得到输出的值。 2. 生成模型的基本概念(1) 生成模型中最基本的概念就是数据样本的分布。利用这个分布我们可以生成数据样本，也可以利用条件概率的方法得到一个条件概率密度函数来做统计推断。 (2) 统计推断在生成模型的方法中，我们常常要利用数据样本的分布来进行推断。通常的方法是利用条件概率公式: $$p(Y|X) = \frac{p(X,Y)}{p(X)}$$ 而P(X)在通常的模型中被作为一个归一化因子。所以需要求的模型就是 P(X,Y).只要得到了 P(X,Y)，我们就可以很轻易的推断出P(Y|X). (3) 生成模型中的基本模型 Wikipedia Gaussian mixture model Hidden Markov model Probabilistic context-free grammar Naive Bayes Averaged one-dependence estimators Latent Dirichlet allocation Restricted Boltzmann machine Generative adversarial networks 2.生成对抗网络基本概念生成对抗网络在2014年被Ian Goodfellow所提出。]]></content>
      <categories>
        <category>deeplearning</category>
      </categories>
      <tags>
        <tag>GAN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tensorflow learning]]></title>
    <url>%2F2017%2F03%2F08%2Ftensorflow_learning%2F</url>
    <content type="text"><![CDATA[tensorflow learningPart 1: Prepare dataThis is common for all deep learning work. And it depends on the data. In python, many people like to use pickle to store data. import tensorflow as tf from tensorflow.examples.tutorials.mnist import input_data mnist = input_data.read_data_sets(&quot;MNIST_data/&quot;,one_hot = True) In general, data include inputs and labels. Part 2: build modelAt most time, we could create a python file named model.py to define our deep learning model. In general, the model include the input layers, hidden layers and loss function. We use placeholder to set the input layers in tensorflow. (Take mnist as example) inputs = tf.placeholder(dtype=&apos;float32&apos;,shape=[None,784],name=&apos;inputs&apos;) labels = tf.placeholder(dtype=&apos;float32&apos;,shape=[None,10],name=&apos;labels&apos;) And we define every layers by the code below: in_size = 784 hid_size = 10 with tf.name_scope(&apos;layer1&apos;): Weights= tf.Variable(tf.random_normal([in_size,hid_size]),name=&apos;Weights&apos;) biases = tf.Variable(tf.random_normal(1,hid_size),name = &apos;biases&apos;) l1 = tf.add(tf.matmul(inputs,Weights),biases) define loss: loss = tf.loss.mean_square(l1,labels) define summary: tf.summary.scalar(loss,name=&apos;loss&apos;) Part 3: train and savedefine the training sizebatch_size = 100all_size = 50000 define the Sessionwith tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess: initialize the parameterstf.global_variables_initializer().run() merged = tf.summary.merge_all() train_writer = tf.summary.FileWriter(&apos;logs/&apos;,sess.graph) for epoch in range(1000): begin = epoch*batch_size%(all_size-batch_size) end = begin + batch_size batch = inputs[begin:end] print(batch.shape) batch_labels = labels[epoch*batch_size:(epoch+1) * batch_size] input the training data train_summary,loss_value=sess.run([train_step,loss],feed_dict={image_inputs:batch,points:batch_labels}) if (epoch%10==0): train_writer.add_summary(train_summary,epoch) print(loss_value) train_writer.close()]]></content>
      <categories>
        <category>deeplearning_frame</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
</search>
